{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MilindAP/Multi-Link-Prediction-for-mmWave-and-subTHz-frequencies-using-LTC-LSTM-and-Symbolic-Regression/blob/main/Multi_Link_Prediction_LTC_Based_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MzI5DAwmtLm",
        "outputId": "408f51e2-b45f-420f-e97e-0b577f0694df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LewX-zsp__5Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "Data1 = pd.read_csv(\"/content/drive/MyDrive/My docs/MultiLink Prediction/data.csv\")\n",
        "#Data1 = Data['SNR']\n",
        "#Data1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data1.to_csv('Data1.csv')\n",
        "Data1.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "aeHRUkNpBfAM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "XNrvZ7E9Bm7x",
        "outputId": "a54bd3d5-cb90-4064-980e-2ab84e6a2b53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Received Power (dBm)  RMS Delay Spread (ns)        SNR  NA4  NA5  NA6  \\\n",
              "0                    -77.5                    8.8  -8.611111  NaN  NaN  NaN   \n",
              "1                    -89.2                    3.0  -9.911111  NaN  NaN  NaN   \n",
              "2                    -89.4                   11.9  -9.933333  NaN  NaN  NaN   \n",
              "3                    -89.5                   12.1  -9.944444  NaN  NaN  NaN   \n",
              "4                    -81.3                    3.9  -9.033333  NaN  NaN  NaN   \n",
              "...                    ...                    ...        ...  ...  ...  ...   \n",
              "2832                -167.0                   31.9 -18.555556  NaN  NaN  NaN   \n",
              "2833                -167.0                    4.4 -18.555556  NaN  NaN  NaN   \n",
              "2834                -167.0                    4.3 -18.555556  NaN  NaN  NaN   \n",
              "2835                -167.0                   13.3 -18.555556  NaN  NaN  NaN   \n",
              "2836                -167.0                   28.8 -18.555556  NaN  NaN  NaN   \n",
              "\n",
              "      NA7  NA8  NA9  NA10  NA11  NA12  NA13  NA14  NA15  NA16  NA17  NA18  \n",
              "0     NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "1     NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "2     NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "3     NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "4     NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "...   ...  ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
              "2832  NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "2833  NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "2834  NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "2835  NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "2836  NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
              "\n",
              "[2837 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d592048-8123-4d73-89bd-1876d20a9905\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Received Power (dBm)</th>\n",
              "      <th>RMS Delay Spread (ns)</th>\n",
              "      <th>SNR</th>\n",
              "      <th>NA4</th>\n",
              "      <th>NA5</th>\n",
              "      <th>NA6</th>\n",
              "      <th>NA7</th>\n",
              "      <th>NA8</th>\n",
              "      <th>NA9</th>\n",
              "      <th>NA10</th>\n",
              "      <th>NA11</th>\n",
              "      <th>NA12</th>\n",
              "      <th>NA13</th>\n",
              "      <th>NA14</th>\n",
              "      <th>NA15</th>\n",
              "      <th>NA16</th>\n",
              "      <th>NA17</th>\n",
              "      <th>NA18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-77.5</td>\n",
              "      <td>8.8</td>\n",
              "      <td>-8.611111</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-89.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-9.911111</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-89.4</td>\n",
              "      <td>11.9</td>\n",
              "      <td>-9.933333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-89.5</td>\n",
              "      <td>12.1</td>\n",
              "      <td>-9.944444</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-81.3</td>\n",
              "      <td>3.9</td>\n",
              "      <td>-9.033333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2832</th>\n",
              "      <td>-167.0</td>\n",
              "      <td>31.9</td>\n",
              "      <td>-18.555556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2833</th>\n",
              "      <td>-167.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>-18.555556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2834</th>\n",
              "      <td>-167.0</td>\n",
              "      <td>4.3</td>\n",
              "      <td>-18.555556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2835</th>\n",
              "      <td>-167.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>-18.555556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>-167.0</td>\n",
              "      <td>28.8</td>\n",
              "      <td>-18.555556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2837 rows × 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d592048-8123-4d73-89bd-1876d20a9905')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d592048-8123-4d73-89bd-1876d20a9905 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d592048-8123-4d73-89bd-1876d20a9905');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00c19fd5-a41c-4c16-aee2-272a90a30abf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00c19fd5-a41c-4c16-aee2-272a90a30abf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00c19fd5-a41c-4c16-aee2-272a90a30abf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d100304b-bdd2-4a69-a16b-1a282a0d64e2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Data1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d100304b-bdd2-4a69-a16b-1a282a0d64e2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Data1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Data1",
              "summary": "{\n  \"name\": \"Data1\",\n  \"rows\": 2837,\n  \"fields\": [\n    {\n      \"column\": \"Received Power (dBm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.97120398446807,\n        \"min\": -167.0,\n        \"max\": -68.3,\n        \"num_unique_values\": 813,\n        \"samples\": [\n          -144.3,\n          -94.1,\n          -99.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMS Delay Spread (ns)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.30381055368131,\n        \"min\": 0.0,\n        \"max\": 178.8,\n        \"num_unique_values\": 764,\n        \"samples\": [\n          65.5,\n          51.3,\n          84.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SNR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4412448886178204,\n        \"min\": -18.55555556,\n        \"max\": -7.588888889,\n        \"num_unique_values\": 813,\n        \"samples\": [\n          -16.03333333,\n          -10.45555556,\n          -11.02222222\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA14\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA15\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA16\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA17\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NA18\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iunp_ycRYVgF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from enum import Enum\n",
        "\n",
        "class MappingType(Enum):\n",
        "    Identity = 0\n",
        "    Linear = 1\n",
        "    Affine = 2\n",
        "\n",
        "class ODESolver(Enum):\n",
        "    SemiImplicit = 0\n",
        "    Explicit = 1\n",
        "    RungeKutta = 2\n",
        "\n",
        "class LTCCell(tf.compat.v1.nn.rnn_cell.RNNCell):\n",
        "\n",
        "    def __init__(self, num_units):\n",
        "\n",
        "        self._input_size = -1\n",
        "        self._num_units = num_units\n",
        "        self._is_built = False\n",
        "\n",
        "        # Number of ODE solver steps in one RNN step\n",
        "        self._ode_solver_unfolds = 6\n",
        "        self._solver = ODESolver.SemiImplicit\n",
        "\n",
        "        self._input_mapping = MappingType.Affine\n",
        "\n",
        "        self._erev_init_factor = 1\n",
        "\n",
        "        self._w_init_max = 1.0\n",
        "        self._w_init_min = 0.01\n",
        "        self._cm_init_min = 0.5\n",
        "        self._cm_init_max = 0.5\n",
        "        self._gleak_init_min = 1\n",
        "        self._gleak_init_max = 1\n",
        "\n",
        "        self._w_min_value = 0.00001\n",
        "        self._w_max_value = 1000\n",
        "        self._gleak_min_value = 0.00001\n",
        "        self._gleak_max_value = 1000\n",
        "        self._cm_t_min_value = 0.000001\n",
        "        self._cm_t_max_value = 1000\n",
        "\n",
        "        self._fix_cm = None\n",
        "        self._fix_gleak = None\n",
        "        self._fix_vleak = None\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def _map_inputs(self,inputs,resuse_scope=False):\n",
        "        varscope = \"sensory_mapping\"\n",
        "        reuse = tf.compat.v1.AUTO_REUSE\n",
        "        if(resuse_scope):\n",
        "            varscope = self._sensory_varscope\n",
        "            reuse = True\n",
        "            tf.reset_default_graph()\n",
        "\n",
        "        with tf.compat.v1.variable_scope(varscope,reuse=reuse) as scope:\n",
        "            self._sensory_varscope = scope\n",
        "            if(self._input_mapping == MappingType.Affine or self._input_mapping == MappingType.Linear):\n",
        "                w =  tf.compat.v1.get_variable(name='input_w',shape=[self._input_size],trainable=True,initializer=tf.initializers.constant(1),reuse=True)\n",
        "                inputs = inputs * w\n",
        "            if(self._input_mapping == MappingType.Affine):\n",
        "                b =  tf.compat.v1.get_variable(name='input_b',shape=[self._input_size],trainable=True,initializer=tf.initializers.constant(0),reuse=True)\n",
        "                inputs = inputs + b\n",
        "        return inputs\n",
        "\n",
        "    # TODO: Implement RNNLayer properly,i.e, allocate variables here\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        with tf.compat.v1.variable_scope(\"ltc\"):\n",
        "            if(not self._is_built):\n",
        "                # TODO: Move this part into the build method inherited form tf.Layers\n",
        "                self._is_built = True\n",
        "                self._input_size = int(inputs.shape[-1])\n",
        "\n",
        "                self._get_variables()\n",
        "\n",
        "            elif(self._input_size != int(inputs.shape[-1])):\n",
        "                raise ValueError(\"You first feed an input with {} features and now one with {} features, that is not possible\".format(\n",
        "                    self._input_size,\n",
        "                    int(inputs[-1])\n",
        "                ))\n",
        "\n",
        "            inputs = self._map_inputs(inputs)\n",
        "\n",
        "            if(self._solver == ODESolver.Explicit):\n",
        "                next_state = self._ode_step_explicit(inputs,state,_ode_solver_unfolds=self._ode_solver_unfolds)\n",
        "            elif(self._solver == ODESolver.SemiImplicit):\n",
        "                next_state = self._ode_step(inputs,state)\n",
        "            elif(self._solver == ODESolver.RungeKutta):\n",
        "                next_state = self._ode_step_runge_kutta(inputs,state)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown ODE solver '{}'\".format(str(self._solver)))\n",
        "\n",
        "            outputs = next_state\n",
        "\n",
        "        return outputs, next_state\n",
        "\n",
        "    # Create tf variables\n",
        "    def _get_variables(self):\n",
        "        self.sensory_mu = tf.compat.v1.get_variable(name='sensory_mu',shape=[self._input_size,self._num_units],trainable=True,initializer=tf.initializers.random_uniform(minval=0.3,maxval=0.8))\n",
        "        self.sensory_sigma = tf.compat.v1.get_variable(name='sensory_sigma',shape=[self._input_size,self._num_units],trainable=True,initializer=tf.initializers.random_uniform(minval=3.0,maxval=8.0))\n",
        "        self.sensory_W = tf.compat.v1.get_variable(name='sensory_W',shape=[self._input_size,self._num_units],trainable=True,initializer=tf.initializers.constant(np.random.uniform(low=self._w_init_min,high=self._w_init_max,size=[self._input_size,self._num_units])))\n",
        "        sensory_erev_init = 2*np.random.randint(low=0,high=2,size=[self._input_size,self._num_units])-1\n",
        "        self.sensory_erev = tf.compat.v1.get_variable(name='sensory_erev',shape=[self._input_size,self._num_units],trainable=True,initializer=tf.initializers.constant(sensory_erev_init*self._erev_init_factor))\n",
        "\n",
        "        self.mu = tf.compat.v1.get_variable(name='mu',shape=[self._num_units,self._num_units],trainable=True,initializer=tf.initializers.random_uniform(minval=0.3,maxval=0.8))\n",
        "        self.sigma = tf.compat.v1.get_variable(name='sigma',shape=[self._num_units,self._num_units],trainable=True,initializer=tf.initializers.random_uniform(minval=3.0,maxval=8.0))\n",
        "        self.W = tf.compat.v1.get_variable(name='W',shape=[self._num_units,self._num_units],trainable=True,initializer=tf.initializers.constant(np.random.uniform(low=self._w_init_min,high=self._w_init_max,size=[self._num_units,self._num_units])))\n",
        "\n",
        "        erev_init = 2*np.random.randint(low=0,high=2,size=[self._num_units,self._num_units])-1\n",
        "        self.erev = tf.compat.v1.get_variable(name='erev',shape=[self._num_units,self._num_units],trainable=True,initializer=tf.initializers.constant(erev_init*self._erev_init_factor))\n",
        "\n",
        "        if(self._fix_vleak is None):\n",
        "            self.vleak = tf.compat.v1.get_variable(name='vleak',shape=[self._num_units],trainable=True,initializer=tf.initializers.random_uniform(minval=-0.2,maxval=0.2))\n",
        "        else:\n",
        "            self.vleak = tf.compat.v1.get_variable(name='vleak',shape=[self._num_units],trainable=False,initializer=tf.initializers.constant(self._fix_vleak))\n",
        "\n",
        "        if(self._fix_gleak is None):\n",
        "            initializer=tf.initializers.constant(self._gleak_init_min)\n",
        "            if(self._gleak_init_max > self._gleak_init_min):\n",
        "                initializer = tf.initializers.random_uniform(minval= self._gleak_init_min,maxval = self._gleak_init_max)\n",
        "            self.gleak = tf.compat.v1.get_variable(name='gleak',shape=[self._num_units],trainable=True,initializer=initializer)\n",
        "        else:\n",
        "            self.gleak = tf.compat.v1.get_variable(name='gleak',shape=[self._num_units],trainable=False,initializer=tf.initializers.constant(self._fix_gleak))\n",
        "\n",
        "        if(self._fix_cm is None):\n",
        "            initializer=tf.initializers.constant(self._cm_init_min)\n",
        "            if(self._cm_init_max > self._cm_init_min):\n",
        "                initializer = tf.initializers.random_uniform(minval= self._cm_init_min,maxval = self._cm_init_max)\n",
        "            self.cm_t = tf.compat.v1.get_variable(name='cm_t',shape=[self._num_units],trainable=True,initializer=initializer)\n",
        "        else:\n",
        "            self.cm_t = tf.compat.v1.get_variable(name='cm_t',shape=[self._num_units],trainable=False,initializer=tf.initializers.constant(self._fix_cm))\n",
        "\n",
        "    # Hybrid euler method\n",
        "    def _ode_step(self,inputs,state):\n",
        "        v_pre = state\n",
        "\n",
        "        sensory_w_activation = self.sensory_W*self._sigmoid(inputs,self.sensory_mu,self.sensory_sigma)\n",
        "        sensory_rev_activation = sensory_w_activation*self.sensory_erev\n",
        "\n",
        "        w_numerator_sensory = tf.reduce_sum(sensory_rev_activation,axis=1)\n",
        "        w_denominator_sensory = tf.reduce_sum(sensory_w_activation,axis=1)\n",
        "\n",
        "        for t in range(self._ode_solver_unfolds):\n",
        "            w_activation = self.W*self._sigmoid(v_pre,self.mu,self.sigma)\n",
        "\n",
        "            rev_activation = w_activation*self.erev\n",
        "\n",
        "            w_numerator = tf.reduce_sum(rev_activation,axis=1) + w_numerator_sensory\n",
        "            w_denominator = tf.reduce_sum(w_activation,axis=1) + w_denominator_sensory\n",
        "\n",
        "            numerator = self.cm_t * v_pre + self.gleak*self.vleak + w_numerator\n",
        "            denominator = self.cm_t + self.gleak + w_denominator\n",
        "\n",
        "            v_pre = numerator/denominator\n",
        "\n",
        "        return v_pre\n",
        "\n",
        "    def _f_prime(self,inputs,state):\n",
        "        v_pre = state\n",
        "\n",
        "        # We can pre-compute the effects of the sensory neurons here\n",
        "        sensory_w_activation = self.sensory_W*self._sigmoid(inputs,self.sensory_mu,self.sensory_sigma)\n",
        "        w_reduced_sensory = tf.reduce_sum(sensory_w_activation,axis=1)\n",
        "\n",
        "        # Unfold the mutliply ODE multiple times into one RNN step\n",
        "        w_activation = self.W*self._sigmoid(v_pre,self.mu,self.sigma)\n",
        "\n",
        "        w_reduced_synapse = tf.reduce_sum(w_activation,axis=1)\n",
        "\n",
        "        sensory_in = self.sensory_erev * sensory_w_activation\n",
        "        synapse_in = self.erev * w_activation\n",
        "\n",
        "        sum_in = tf.reduce_sum(sensory_in,axis=1) - v_pre*w_reduced_synapse + tf.reduce_sum(synapse_in,axis=1) - v_pre * w_reduced_sensory\n",
        "\n",
        "        f_prime = 1/self.cm_t * (self.gleak * (self.vleak-v_pre) + sum_in)\n",
        "\n",
        "        return f_prime\n",
        "\n",
        "    def _ode_step_runge_kutta(self,inputs,state):\n",
        "\n",
        "        h = 0.1\n",
        "        for i in range(self._ode_solver_unfolds):\n",
        "            k1 = h*self._f_prime(inputs,state)\n",
        "            k2 = h*self._f_prime(inputs,state+k1*0.5)\n",
        "            k3 = h*self._f_prime(inputs,state+k2*0.5)\n",
        "            k4 = h*self._f_prime(inputs,state+k3)\n",
        "\n",
        "            state = state + 1.0/6*(k1+2*k2+2*k3+k4)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _ode_step_explicit(self,inputs,state,_ode_solver_unfolds):\n",
        "        v_pre = state\n",
        "\n",
        "        # We can pre-compute the effects of the sensory neurons here\n",
        "        sensory_w_activation = self.sensory_W*self._sigmoid(inputs,self.sensory_mu,self.sensory_sigma)\n",
        "        w_reduced_sensory = tf.reduce_sum(sensory_w_activation,axis=1)\n",
        "\n",
        "\n",
        "        # Unfold the mutliply ODE multiple times into one RNN step\n",
        "        for t in range(_ode_solver_unfolds):\n",
        "            w_activation = self.W*self._sigmoid(v_pre,self.mu,self.sigma)\n",
        "\n",
        "            w_reduced_synapse = tf.reduce_sum(w_activation,axis=1)\n",
        "\n",
        "            sensory_in = self.sensory_erev * sensory_w_activation\n",
        "            synapse_in = self.erev * w_activation\n",
        "\n",
        "            sum_in = tf.reduce_sum(sensory_in,axis=1) - v_pre*w_reduced_synapse + tf.reduce_sum(synapse_in,axis=1) - v_pre * w_reduced_sensory\n",
        "\n",
        "            f_prime = 1/self.cm_t * (self.gleak * (self.vleak-v_pre) + sum_in)\n",
        "\n",
        "            v_pre = v_pre + 0.1 * f_prime\n",
        "\n",
        "        return v_pre\n",
        "\n",
        "    def _sigmoid(self,v_pre,mu,sigma):\n",
        "        v_pre = tf.reshape(v_pre,[-1,v_pre.shape[-1],1])\n",
        "        mues = v_pre - mu\n",
        "        x = sigma*mues\n",
        "        return tf.nn.sigmoid(x)\n",
        "\n",
        "    def get_param_constrain_op(self):\n",
        "\n",
        "        cm_clipping_op = tf.compat.v1.assign(self.cm_t,tf.clip_by_value(self.cm_t, self._cm_t_min_value, self._cm_t_max_value))\n",
        "        gleak_clipping_op = tf.compat.v1.assign(self.gleak,tf.clip_by_value(self.gleak, self._gleak_min_value, self._gleak_max_value))\n",
        "        w_clipping_op = tf.compat.v1.assign(self.W,tf.clip_by_value(self.W, self._w_min_value, self._w_max_value))\n",
        "        sensory_w_clipping_op = tf.compat.v1.assign(self.sensory_W ,tf.clip_by_value(self.sensory_W, self._w_min_value, self._w_max_value))\n",
        "\n",
        "        return [cm_clipping_op,gleak_clipping_op,w_clipping_op,sensory_w_clipping_op]\n",
        "\n",
        "    def export_weights(self,dirname,sess,output_weights=None):\n",
        "        os.makedirs(dirname,exist_ok=True)\n",
        "        w,erev,mu,sigma = sess.run([self.W,self.erev,self.mu,self.sigma])\n",
        "        sensory_w,sensory_erev,sensory_mu,sensory_sigma = sess.run([self.sensory_W,self.sensory_erev,self.sensory_mu,self.sensory_sigma])\n",
        "        vleak,gleak,cm = sess.run([self.vleak,self.gleak,self.cm_t])\n",
        "\n",
        "        if(not output_weights is None):\n",
        "            output_w,output_b = sess.run(output_weights)\n",
        "            np.savetxt(os.path.join(dirname,\"output_w.csv\"),output_w)\n",
        "            np.savetxt(os.path.join(dirname,\"output_b.csv\"),output_b)\n",
        "        np.savetxt(os.path.join(dirname,\"w.csv\"),w)\n",
        "        np.savetxt(os.path.join(dirname,\"erev.csv\"),erev)\n",
        "        np.savetxt(os.path.join(dirname,\"mu.csv\"),mu)\n",
        "        np.savetxt(os.path.join(dirname,\"sigma.csv\"),sigma)\n",
        "        np.savetxt(os.path.join(dirname,\"sensory_w.csv\"),sensory_w)\n",
        "        np.savetxt(os.path.join(dirname,\"sensory_erev.csv\"),sensory_erev)\n",
        "        np.savetxt(os.path.join(dirname,\"sensory_mu.csv\"),sensory_mu)\n",
        "        np.savetxt(os.path.join(dirname,\"sensory_sigma.csv\"),sensory_sigma)\n",
        "        np.savetxt(os.path.join(dirname,\"vleak.csv\"),vleak)\n",
        "        np.savetxt(os.path.join(dirname,\"gleak.csv\"),gleak)\n",
        "        np.savetxt(os.path.join(dirname,\"cm.csv\"),cm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy  as np\n",
        "import os\n",
        "\n",
        "\n",
        "class CTRNN(tf.compat.v1.nn.rnn_cell.RNNCell):\n",
        "\n",
        "    def __init__(self, num_units,cell_clip=-1,global_feedback=False,fix_tau=True):\n",
        "        self._num_units = num_units\n",
        "        # Number of ODE solver steps\n",
        "        self._unfolds = 6\n",
        "        # Time of each ODE solver step, for variable time RNN change this\n",
        "        # to a placeholder/non-trainable variable\n",
        "        self._delta_t = 0.1\n",
        "\n",
        "        self.global_feedback = global_feedback\n",
        "\n",
        "        # Time-constant of the cell\n",
        "        self.fix_tau = fix_tau\n",
        "        self.tau = 1\n",
        "        self.cell_clip = cell_clip\n",
        "\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def export_weights(self,dirname,sess,output_weights=None):\n",
        "        os.makedirs(dirname,exist_ok=True)\n",
        "        w,b = sess.run([self.W,self.b])\n",
        "\n",
        "        tau = np.ones(1)\n",
        "        if(not self.fix_tau):\n",
        "            sp_op = tf.nn.softplus(self._tau_var)\n",
        "            tau = sess.run(sp_op)\n",
        "        if(not output_weights is None):\n",
        "            output_w,output_b = sess.run(output_weights)\n",
        "            np.savetxt(os.path.join(dirname,\"output_w.csv\"),output_w)\n",
        "            np.savetxt(os.path.join(dirname,\"output_b.csv\"),output_b)\n",
        "        np.savetxt(os.path.join(dirname,\"w.csv\"),w)\n",
        "        np.savetxt(os.path.join(dirname,\"b.csv\"),b)\n",
        "        np.savetxt(os.path.join(dirname,\"tau.csv\"),b)\n",
        "\n",
        "    # TODO: Implement RNNLayer properly,i.e, allocate variables here\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def _dense(self,units,inputs,activation,name,bias_initializer=tf.constant_initializer(0.0)):\n",
        "        input_size = int(inputs.shape[-1])\n",
        "        W = tf.get_variable('W_{}'.format(name), [input_size, units])\n",
        "        b = tf.get_variable(\"bias_{}\".format(name), [units],initializer=bias_initializer)\n",
        "\n",
        "        y = tf.matmul(inputs,W) + b\n",
        "        if(not activation is None):\n",
        "            y = activation(y)\n",
        "\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        return y\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        # CTRNN ODE is: df/dt = NN(x) - f\n",
        "        # where x is the input, and NN is a MLP.\n",
        "        # Input could be: 1: just the input of the RNN cell\n",
        "        # or 2: input of the RNN cell merged with the current state\n",
        "\n",
        "        self._input_size = int(inputs.shape[-1])\n",
        "        with tf.variable_scope(scope or type(self).__name__):\n",
        "          #with tf.variable_scope('lstm', reuse=True) as scope:\n",
        "            with tf.variable_scope(\"rnn\",reuse=tf.AUTO_REUSE):  # Reset gate and update gate.\n",
        "                if(not self.fix_tau):\n",
        "                    tau = tf.get_variable('tau', [],initializer=tf.constant_initializer(self.tau))\n",
        "                    self._tau_var = tau\n",
        "                    tau = tf.nn.softplus(tau) # Make sure tau is positive\n",
        "                else:\n",
        "                    tau = self.tau\n",
        "\n",
        "                # Input Option 1: RNNCell input\n",
        "                if(not self.global_feedback):\n",
        "                    input_f_prime = self._dense(units=self._num_units,inputs=inputs,activation=tf.nn.tanh,name=\"step\")\n",
        "                for i in range(self._unfolds):\n",
        "                    # Input Option 2: RNNCell input AND RNN state\n",
        "                    if(self.global_feedback):\n",
        "                        fused_input = tf.concat([inputs,state],axis=-1)\n",
        "                        input_f_prime = self._dense(units=self._num_units,inputs=fused_input,activation=tf.nn.tanh,name=\"step\")\n",
        "\n",
        "                    # df/dt\n",
        "                    f_prime = -state/self.tau + input_f_prime\n",
        "\n",
        "                    # If we solve this ODE with explicit euler we get\n",
        "                    # f(t+deltaT) = f(t) + deltaT * df/dt\n",
        "                    state = state + self._delta_t * f_prime\n",
        "\n",
        "                    # Optional clipping of the RNN cell to enforce stability (not needed)\n",
        "                    if(self.cell_clip > 0):\n",
        "                        state = tf.clip_by_value(state,-self.cell_clip,self.cell_clip)\n",
        "\n",
        "        return state,state\n",
        "\n",
        "\n",
        "class NODE(tf.compat.v1.nn.rnn_cell.RNNCell):\n",
        "\n",
        "    def __init__(self, num_units,cell_clip=-1):\n",
        "        self._num_units = num_units\n",
        "        # Number of ODE solver steps\n",
        "        self._unfolds = 6\n",
        "        # Time of each ODE solver step, for variable time RNN change this\n",
        "        # to a placeholder/non-trainable variable\n",
        "        self._delta_t = 0.1\n",
        "\n",
        "        self.cell_clip = cell_clip\n",
        "\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def export_weights(self,dirname,sess,output_weights=None):\n",
        "        os.makedirs(dirname,exist_ok=True)\n",
        "        w,b = sess.run([self.W,self.b])\n",
        "\n",
        "        tau = np.ones(1)\n",
        "        if(not self.fix_tau):\n",
        "            sp_op = tf.nn.softplus(self._tau_var)\n",
        "            tau = sess.run(sp_op)\n",
        "        if(not output_weights is None):\n",
        "            output_w,output_b = sess.run(output_weights)\n",
        "            np.savetxt(os.path.join(dirname,\"output_w.csv\"),output_w)\n",
        "            np.savetxt(os.path.join(dirname,\"output_b.csv\"),output_b)\n",
        "        np.savetxt(os.path.join(dirname,\"w.csv\"),w)\n",
        "        np.savetxt(os.path.join(dirname,\"b.csv\"),b)\n",
        "        np.savetxt(os.path.join(dirname,\"tau.csv\"),b)\n",
        "\n",
        "    # TODO: Implement RNNLayer properly,i.e, allocate variables here\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def _ode_step_runge_kutta(self,inputs,state):\n",
        "\n",
        "        for i in range(self._unfolds):\n",
        "            k1 = self._delta_t*self._f_prime(inputs,state)\n",
        "            k2 = self._delta_t*self._f_prime(inputs,state+k1*0.5)\n",
        "            k3 = self._delta_t*self._f_prime(inputs,state+k2*0.5)\n",
        "            k4 = self._delta_t*self._f_prime(inputs,state+k3)\n",
        "\n",
        "            state = state + (k1+2*k2+2*k3+k4)/6.0\n",
        "\n",
        "            # Optional clipping of the RNN cell to enforce stability (not needed)\n",
        "            if(self.cell_clip > 0):\n",
        "                state = tf.clip_by_value(state,-self.cell_clip,self.cell_clip)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _f_prime(self,inputs,state):\n",
        "        fused_input = tf.concat([inputs,state],axis=-1)\n",
        "        input_f_prime = self._dense(units=self._num_units,inputs=fused_input,activation=tf.nn.tanh,name=\"step\")\n",
        "        return input_f_prime\n",
        "\n",
        "    def _dense(self,units,inputs,activation,name,bias_initializer=tf.constant_initializer(0.0)):\n",
        "        input_size = int(inputs.shape[-1])\n",
        "        W = tf.get_variable('W_{}'.format(name), [input_size, units])\n",
        "        b = tf.get_variable(\"bias_{}\".format(name), [units],initializer=bias_initializer)\n",
        "\n",
        "        y = tf.matmul(inputs,W) + b\n",
        "        if(not activation is None):\n",
        "            y = activation(y)\n",
        "\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        return y\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        # CTRNN ODE is: df/dt = NN(x) - f\n",
        "        # where x is the input, and NN is a MLP.\n",
        "        # Input could be: 1: just the input of the RNN cell\n",
        "        # or 2: input of the RNN cell merged with the current state\n",
        "\n",
        "        self._input_size = int(inputs.shape[-1])\n",
        "        with tf.variable_scope(scope or type(self).__name__):\n",
        "            with tf.variable_scope(\"RNN\",reuse=tf.AUTO_REUSE):  # Reset gate and update gate.\n",
        "\n",
        "                state = self._ode_step_runge_kutta(inputs,state)\n",
        "\n",
        "        return state,state\n",
        "\n",
        "\n",
        "\n",
        "class CTGRU(tf.compat.v1.nn.rnn_cell.RNNCell):\n",
        "    # https://arxiv.org/abs/1710.04110\n",
        "    def __init__(self, num_units,M=8,cell_clip=-1):\n",
        "        self._num_units = num_units\n",
        "        self.M = M\n",
        "        self.cell_clip = cell_clip\n",
        "        self.ln_tau_table = np.empty(self.M)\n",
        "        tau = 1\n",
        "        for i in range(self.M):\n",
        "            self.ln_tau_table[i] = np.log(tau)\n",
        "            tau = tau * (10.0**0.5)\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._num_units*self.M\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    # TODO: Implement RNNLayer properly,i.e, allocate variables here\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def _dense(self,units,inputs,activation,name,bias_initializer=tf.constant_initializer(0.0)):\n",
        "        input_size = int(inputs.shape[-1])\n",
        "        W = tf.get_variable('W_{}'.format(name), [input_size, units])\n",
        "        b = tf.get_variable(\"bias_{}\".format(name), [units],initializer=bias_initializer)\n",
        "\n",
        "        y = tf.matmul(inputs,W) + b\n",
        "        if(not activation is None):\n",
        "            y = activation(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        self._input_size = int(inputs.shape[1])\n",
        "\n",
        "        # CT-GRU input is actually a matrix and not a vector\n",
        "        h_hat = tf.reshape(state,[-1,self._num_units,self.M])\n",
        "        h = tf.reduce_sum(h_hat,axis=2)\n",
        "        state = None # Set state to None, to avoid misuses (bugs) in the code below\n",
        "\n",
        "        with tf.variable_scope(scope or type(self).__name__):\n",
        "            with tf.variable_scope(\"Gates\"):  # Reset gate and update gate.\n",
        "                fused_input = tf.concat([inputs,h],axis=-1)\n",
        "                ln_tau_r = tf.layers.Dense(self._num_units*self.M,activation=None,name=\"tau_r\")(fused_input)\n",
        "                ln_tau_r = tf.reshape(ln_tau_r,shape=[-1,self._num_units,self.M])\n",
        "                sf_input_r = -tf.square(ln_tau_r-self.ln_tau_table)\n",
        "                rki = tf.nn.softmax(logits=sf_input_r,axis=2)\n",
        "\n",
        "                q_input = tf.reduce_sum(rki*h_hat,axis=2)\n",
        "                reset_value = tf.concat([inputs,q_input],axis=1)\n",
        "                qk = self._dense(units=self._num_units,inputs=reset_value,activation=tf.nn.tanh,name=\"detect_signal\")\n",
        "\n",
        "                qk = tf.reshape(qk,[-1,self._num_units,1]) # in order to broadcast\n",
        "\n",
        "                ln_tau_s = tf.layers.Dense(self._num_units*self.M,activation=None,name=\"tau_s\")(fused_input)\n",
        "                ln_tau_s = tf.reshape(ln_tau_s,shape=[-1,self._num_units,self.M])\n",
        "                sf_input_s = -tf.square(ln_tau_s-self.ln_tau_table)\n",
        "                ski = tf.nn.softmax(logits=sf_input_s,axis=2)\n",
        "\n",
        "                h_hat_next = ((1-ski)*h_hat + ski*qk)*np.exp(-1.0/self.ln_tau_table)\n",
        "\n",
        "                if(self.cell_clip > 0):\n",
        "                    h_hat_next = tf.clip_by_value(h_hat_next,-self.cell_clip,self.cell_clip)\n",
        "                # Compute new state\n",
        "                h_next = tf.reduce_sum(h_hat_next,axis=2)\n",
        "                h_hat_next_flat = tf.reshape(h_hat_next,shape=[-1,self._num_units*self.M])\n",
        "\n",
        "        return h_next, h_hat_next_flat"
      ],
      "metadata": {
        "id": "iO5KQ77GGxNy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Run on CPU\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "#import ltc_model as ltc\n",
        "#from ctrnn_model import CTRNN, NODE, CTGRU\n",
        "import argparse\n",
        "import datetime as dt\n",
        "#import keras\n",
        "\n",
        "\n",
        "def cut_in_sequences(x,seq_len,inc=1):\n",
        "    # given a file, produces sequences of length seq_len incrementing with inc. Basically generates sequences of data\n",
        "    sequences_x = []\n",
        "    sequences_y = []\n",
        "\n",
        "    for s in range(0,x.shape[0] - seq_len-1,inc):\n",
        "        start = s\n",
        "        end = start+seq_len\n",
        "        sequences_x.append(x[start:end])\n",
        "        sequences_y.append(x[start+1:end+1])\n",
        "\n",
        "    return sequences_x,sequences_y\n",
        "\n",
        "class ENSO_data:\n",
        "\n",
        "    def __init__(self,seq_len=32):\n",
        "\n",
        "        data1 = pd.read_csv(\"/content/Data1.csv\")\n",
        "        #data = pd.read_csv(\"D:\\Reasearch projects\\LTC for ENSO\\data\\ENSO.csv\")\n",
        "        #data1 = np.array(data).reshape(-1, 1)\n",
        "        data_processed = self.process_data(data1)\n",
        "        #self.train_x,self.test_x,self.valid_x,self.train_y,self.test_y,self.valid_y = self.split_data(data_processed)\n",
        "\n",
        "        self.train_x,self.train_y = self.load_data(data_processed[:1250, :])\n",
        "        self.test_x,self.test_y = self.load_data(data_processed[1251:2350, :])\n",
        "        self.valid_x,self.valid_y = self.load_data(data_processed[2351:, :])\n",
        "\n",
        "\n",
        "\n",
        "        #self.train_x = self.train_x.reshape(self.train_x.shape[0], self.train_x.shape[1])\n",
        "        #self.test_x = self.test_x.reshape(self.test_x.shape[0], self.test_x.shape[1])\n",
        "        #self.valid_x = self.valid_x.reshape(self.valid_x.shape[0],self.valid_x.shape[1])\n",
        "\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.obs_size = 17\n",
        "\n",
        "    def load_data(self,data): #loads npy files.\n",
        "        all_x = []\n",
        "        all_y = []\n",
        "\n",
        "\n",
        "        #r = r.astype(np.float32)\n",
        "        x,y = cut_in_sequences(data,32,10)\n",
        "\n",
        "        all_x.extend(x)\n",
        "        all_y.extend(y)\n",
        "\n",
        "        test_x = np.stack(all_x,axis=1)\n",
        "        test_y = np.stack(all_y,axis=1)\n",
        "\n",
        "        return np.stack(all_x,axis=1),np.stack(all_y,axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    def split_data(self,data):\n",
        "        #values = data.values\n",
        "        x_train, x_test,x_val,y_train, y_test,y_val = data[:1250, :-1], data[1251:2350, :-1],data[2351:, :-1], data[:1250, -1:], data[1251:2350, -1:],data[2351:, -1:]\n",
        "        return  np.array(x_train), np.array(x_test),np.array(x_val),np.array(y_train), np.array(y_test),np.array(y_val)\n",
        "\n",
        "\n",
        "    def iterate_train(self,batch_size=20):\n",
        "        total_seqs = self.train_x.shape[0]\n",
        "        permutation = np.random.permutation(total_seqs)\n",
        "        total_batches = total_seqs // batch_size\n",
        "\n",
        "        for i in range(total_batches):\n",
        "            start = i*batch_size\n",
        "            end = start + batch_size\n",
        "            batch_x = self.train_x[permutation[start:end], :]\n",
        "            batch_y = self.train_y[permutation[start:end], :]\n",
        "            yield (batch_x,batch_y)\n",
        "\n",
        "    def series_to_supervised(self,data, n_in=1, n_out=1, dropnan=True):\n",
        "\n",
        "        n_vars = 1 if type(data) is list else data.shape[1]\n",
        "        df = pd.DataFrame(data)\n",
        "        cols, names = list(), list()\n",
        "        # input sequence (t-n, ... t-1)\n",
        "        for i in range(n_in, 0, -1):\n",
        "            cols.append(df.shift(i))\n",
        "            names += [('var{}(t-{})'.format(j+1, i)) for j in range(n_vars)]\n",
        "        # forecast sequence (t, t+1, ... t+n)\n",
        "        for i in range(0, n_out):\n",
        "            cols.append(df.shift(-i).iloc[:, -1])\n",
        "            if i == 0:\n",
        "                names += ['var(t)']\n",
        "            else:\n",
        "                names += ['var(t+{})'.format(i)]\n",
        "        # put it all together\n",
        "        agg = pd.concat(cols, axis=1)\n",
        "        agg.columns = names\n",
        "        # drop rows with NaN values\n",
        "        if dropnan:\n",
        "            agg.dropna(inplace=True)\n",
        "        return agg\n",
        "\n",
        "    def process_data(self,data1):\n",
        "        #data1.replace('NAN', np.nan, inplace=True)\n",
        "        #data1.isna().sum(axis=0)\n",
        "        #data.rename(columns={'Precipitation (mm/day)': 'Precipitation'}, inplace=True)\n",
        "        #data1 = [str(i).replace(\",\", \"\") for i in data1]\n",
        "        data1 = data1.astype({'SNR': float})\n",
        "        '''data1 = data1.astype({\n",
        "                'Month': str,\n",
        "                'Season': str,\n",
        "                'SNR': float,\n",
        "                'NINO 1+2 SST': float,\n",
        "                'NINO 1+2 SST Anomalies': float,\n",
        "                'NINO 3 SST': float,\n",
        "                'NINO 3 SST Anomalies': float,\n",
        "                'NINO 3.4 SST': float,\n",
        "                'NINO 3.4 SST Anomalies': float,\n",
        "                'NINO 4 SST': float,\n",
        "                'NINO 4 SST Anomalies': float,\n",
        "                'OLR': float,\n",
        "                'TNI': float,\n",
        "                'Precipitation': float\n",
        "             })'''\n",
        "\n",
        "        #mean_month = data.groupby('Month')[data.columns[4:]].transform('mean')\n",
        "        #data.fillna(mean_month, inplace=True)\n",
        "        #data.set_index('Date', inplace= True)\n",
        "        #indicators = ['ONI', 'NINO 3.4 SST Anomalies', 'OLR', 'TNI', 'PNA', 'Precipitation', 'SOI'] #from correlation matrix\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        scaler = MinMaxScaler(feature_range = (0,1))\n",
        "        df_scaled = scaler.fit_transform(data1)\n",
        "        df_transformed = df_scaled\n",
        "        #df_transformed = self.series_to_supervised(df_scaled, 1, 1)\n",
        "        return df_transformed\n",
        "\n",
        "class ENSO_model:\n",
        "\n",
        "    def __init__(self,model_type,model_size,sparsity_level=0.5,learning_rate = 0.001):\n",
        "        self.model_type = model_type\n",
        "        self.constrain_op = []\n",
        "        self.sparsity_level = sparsity_level\n",
        "        self.x = tf.compat.v1.placeholder(dtype=tf.float32,shape=[None, None, 2])\n",
        "        #self.x = tf.compat.v1.placeholder(dtype=tf.float32, shape=[32, 107, 2])\n",
        "        self.target_y = tf.compat.v1.placeholder(dtype=tf.float32,shape=[None, None, 2])\n",
        "        #self.target_y = tf.compat.v1.placeholder(dtype=tf.float32, shape=[32, 107, 2])\n",
        "\n",
        "        self.model_size = model_size\n",
        "        head = self.x\n",
        "\n",
        "        if(model_type == \"lstm\"):\n",
        "            self.fused_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(model_size)\n",
        "            #tf.compat.v1.reset_default_graph()\n",
        "\n",
        "            head,_ = tf.compat.v1.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
        "        elif(model_type.startswith(\"ltc\")):\n",
        "            learning_rate = 0.01 # LTC needs a higher learning rate\n",
        "            self.wm = ltc.LTCCell(model_size)\n",
        "            if(model_type.endswith(\"_rk\")):\n",
        "                self.wm._solver = ltc.ODESolver.RungeKutta\n",
        "            elif(model_type.endswith(\"_ex\")):\n",
        "                self.wm._solver = ltc.ODESolver.Explicit\n",
        "            else:\n",
        "                self.wm._solver = ltc.ODESolver.SemiImplicit\n",
        "\n",
        "            head,_ = tf.compat.v1.nn.dynamic_rnn(self.wm,head,dtype=tf.float32,time_major=True)\n",
        "            self.constrain_op.extend(self.wm.get_param_constrain_op())\n",
        "        elif(model_type == \"node\"):\n",
        "            self.fused_cell = NODE(model_size,cell_clip=-1)\n",
        "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
        "        elif(model_type == \"ctgru\"):\n",
        "            self.fused_cell = CTGRU(model_size,cell_clip=-1)\n",
        "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
        "        elif(model_type == \"ctrnn\"):\n",
        "            self.fused_cell = CTRNN(model_size,cell_clip=-1,global_feedback=True)\n",
        "            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model type '{}'\".format(model_type))\n",
        "\n",
        "        if(self.sparsity_level > 0):\n",
        "            self.constrain_op.extend(self.get_sparsity_ops())\n",
        "\n",
        "        self.y = tf.compat.v1.layers.dense(head,units=1,activation=None)\n",
        "        #self.y = tf.keras.layers.Dense(head,units=1,activation=None)\n",
        "        print(\"logit shape: \",str(self.y.shape))\n",
        "        self.loss = tf.reduce_mean(tf.square(self.target_y-self.y))\n",
        "        self.rmse = tf.sqrt(self.loss)\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "        self.train_step = optimizer.minimize(self.loss)\n",
        "\n",
        "        self.accuracy = tf.reduce_mean(tf.abs(self.target_y-self.y))\n",
        "\n",
        "        self.sess = tf.compat.v1.InteractiveSession()\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        self.result_file = os.path.join(\"results\",\"Data1\",\"{}_{}_{:02d}.csv\".format(model_type,model_size,int(100*self.sparsity_level)))\n",
        "        if(not os.path.exists(\"/content/results/Data1\")):\n",
        "            os.makedirs(\"/content/results/Data1\")\n",
        "        if(not os.path.isfile(self.result_file)):\n",
        "            with open(self.result_file,\"w\") as f:\n",
        "                f.write(\"best epoch, train loss, train mae, valid loss, valid mae,valid rmse, test loss, test mae,test rmse\\n\")\n",
        "\n",
        "        self.checkpoint_path = os.path.join(\"tf_sessions\",\"cheetah\",\"{}\".format(model_type))\n",
        "        if(not os.path.exists(\"tf_sessions/cheetah\")):\n",
        "            os.makedirs(\"tf_sessions/cheetah\")\n",
        "\n",
        "        self.saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "\n",
        "    def get_sparsity_ops(self):\n",
        "        tf_vars = tf.trainable_variables()\n",
        "        op_list = []\n",
        "        for v in tf_vars:\n",
        "            #print(\"Variable {}\".format(str(v)))\n",
        "            if(v.name.startswith(\"rnn\")):\n",
        "                if(len(v.shape)<2):\n",
        "                    # Don't sparsity biases\n",
        "                    continue\n",
        "                if(\"ltc\" in v.name and (not \"W:0\" in v.name)):\n",
        "                    # LTC can be sparsified by only setting w[i,j] to 0\n",
        "                    # both input and recurrent matrix will be sparsified\n",
        "                    continue\n",
        "            op_list.append(self.sparse_var(v,self.sparsity_level))\n",
        "\n",
        "        return op_list\n",
        "\n",
        "    def sparse_var(self,v,sparsity_level):\n",
        "        mask = np.random.choice([0, 1], size=v.shape, p=[sparsity_level,1-sparsity_level]).astype(np.float32)\n",
        "        v_assign_op = tf.assign(v,v*mask)\n",
        "        print(\"Var[{}] will be sparsified with {:0.2f} sparsity level\".format(\n",
        "            v.name,sparsity_level\n",
        "        ))\n",
        "        return v_assign_op\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        self.saver.save(self.sess, self.checkpoint_path)\n",
        "\n",
        "    def restore(self):\n",
        "        self.saver.restore(self.sess, self.checkpoint_path)\n",
        "\n",
        "\n",
        "    def fit(self,cheetah_data,epochs,verbose=True,log_period=50):\n",
        "\n",
        "        best_valid_loss = np.PINF\n",
        "        best_valid_stats = (0,0,0,0,0,0,0,0,0)\n",
        "        self.save()\n",
        "        for e in range(epochs):\n",
        "            if(verbose and e%log_period == 0):\n",
        "                test_acc,test_loss,test_rmse = self.sess.run([self.accuracy,self.loss,self.rmse],{self.x:cheetah_data.test_x,self.target_y: cheetah_data.test_y})\n",
        "                valid_acc,valid_loss,valid_rmse = self.sess.run([self.accuracy,self.loss,self.rmse],{self.x:cheetah_data.valid_x,self.target_y: cheetah_data.valid_y})\n",
        "                if((valid_loss < best_valid_loss and e > 0) or e==1):\n",
        "                    # print(\"BEST\")\n",
        "                    best_valid_loss = valid_loss\n",
        "                    best_valid_stats = (\n",
        "                        e,\n",
        "                        np.mean(losses),np.mean(accs),\n",
        "                        valid_loss,valid_acc,valid_rmse,\n",
        "                        test_loss,test_acc,test_rmse)\n",
        "                    self.save()\n",
        "\n",
        "            losses = []\n",
        "            accs = []\n",
        "            for batch_x,batch_y in cheetah_data.iterate_train(batch_size=10):\n",
        "                acc,loss,_ = self.sess.run([self.accuracy,self.loss,self.train_step],{self.x:batch_x,self.target_y: batch_y})\n",
        "                if(len(self.constrain_op) > 0):\n",
        "                    self.sess.run(self.constrain_op)\n",
        "\n",
        "                losses.append(loss)\n",
        "                accs.append(acc)\n",
        "\n",
        "            if(verbose and e%log_period == 0):\n",
        "                print(\"Epochs {:03d}, train loss: {:0.2f}, train mae: {:0.2f}, valid loss: {:0.2f}, valid mae: {:0.2f}, test loss: {:0.2f}, test mae: {:0.2f}\".format(\n",
        "                    e,\n",
        "                    np.mean(losses),np.mean(accs),\n",
        "                    valid_loss,valid_acc,valid_rmse,\n",
        "                    test_loss,test_acc,test_rmse\n",
        "                ))\n",
        "            if(e > 0 and (not np.isfinite(np.mean(losses)))):\n",
        "                break\n",
        "        self.restore()\n",
        "        best_epoch,train_loss,train_acc,valid_loss,valid_acc,valid_rmse,test_loss,test_acc,test_rmse = best_valid_stats\n",
        "        print(\"Best epoch {:03d}, train loss: {:0.2f}, train mae: {:0.2f}, valid loss: {:0.2f}, valid mae: {:0.2f},valid rmse: {:0.2f}, test loss: {:0.2f}, test mae: {:0.2f},test rmse: {:0.2f}\".format(\n",
        "            best_epoch,\n",
        "            train_loss,train_acc,\n",
        "            valid_loss,valid_acc,valid_rmse,\n",
        "            test_loss,test_acc,test_rmse\n",
        "        ))\n",
        "        with open(self.result_file,\"a\") as f:\n",
        "            f.write(\"{:08d}, {:0.8f}, {:0.8f}, {:0.8f}, {:0.8f}, {:0.8f}, {:0.8f},{:0.8f},{:0.8f}\\n\".format(\n",
        "            best_epoch,\n",
        "            train_loss,train_acc,\n",
        "            valid_loss,valid_acc,valid_rmse,\n",
        "            test_loss,test_acc,test_rmse\n",
        "        ))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # python3 cheetah.py --size 32 --model lstm --log 1 --epochs 200\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model',default=\"lstm\")\n",
        "    parser.add_argument('--log',default=1,type=int)\n",
        "    parser.add_argument('--size',default=32,type=int)\n",
        "    parser.add_argument('--epochs',default=200,type=int)\n",
        "    parser.add_argument('--sparsity',default=0.0,type=float)\n",
        "    #args = parser.parse_args()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "    traffic_data = ENSO_data()\n",
        "    model = ENSO_model(model_type = args.model,model_size=args.size,sparsity_level=args.sparsity)\n",
        "\n",
        "\n",
        "    model.fit(traffic_data,epochs=args.epochs,log_period=args.log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "4d23cf37-b69a-44d6-992f-a92d6ef4488a",
        "id": "BB96bGwJGxfm"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:473: RuntimeWarning: All-NaN slice encountered\n",
            "  data_min = np.nanmin(X, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:474: RuntimeWarning: All-NaN slice encountered\n",
            "  data_max = np.nanmax(X, axis=0)\n",
            "<ipython-input-7-eb65e19964ab>:161: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  self.fused_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(model_size)\n",
            "WARNING:tensorflow:From <ipython-input-7-eb65e19964ab>:164: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:1043: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "<ipython-input-7-eb65e19964ab>:192: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.y = tf.compat.v1.layers.dense(head,units=1,activation=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logit shape:  (None, None, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot feed value of shape (32, 107, 19) for Tensor Placeholder:0, which has shape (None, None, 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eb65e19964ab>\u001b[0m in \u001b[0;36m<cell line: 306>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-eb65e19964ab>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, cheetah_data, epochs, verbose, log_period)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlog_period\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcheetah_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheetah_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcheetah_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheetah_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[1;32m   1188\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0;32m-> 1189\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0;34mf'Cannot feed value of shape {str(np_val.shape)} for Tensor '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;34mf'{subfeed_t.name}, which has shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (32, 107, 19) for Tensor Placeholder:0, which has shape (None, None, 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(train mae)+1)\n",
        "\n",
        "plt.plot(epochs, train_rmse, 'bo', label='Training RMSE')\n",
        "plt.plot(epochs, test_rmse, 'r', label='Test RMSE')\n",
        "plt.title('Training and Test MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "UjeZgulwmJty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "aaa46db4-3093-47a9-df57-58a0808de39d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-8-f2547a7e68f7>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f2547a7e68f7>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    epochs = range(1, len(train mae)+1)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    }
  ]
}